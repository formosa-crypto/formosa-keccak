require "keccak1600x4_avx2.jinc"


// pack 4 keccak states (st25) into a 4-way state (st4x)
inline fn __u256x4_4u64x4
( reg u256 x0 x1 x2 x3
) -> reg u256, reg u256, reg u256, reg u256 {
  // x0 = l00 l01  l02 l03
  // x1 = l10 l11  l12 l13
  // x2 = l20 l21  l22 l23
  // x3 = l30 l31  l32 l33
  reg u256 y0, y1, y2, y3;
  y0 = #VPUNPCKL_4u64(x0, x1);	// y0 = l00 l10  l02 l12
  y1 = #VPUNPCKH_4u64(x0, x1);	// y1 = l01 l11  l03 l13
  y2 = #VPUNPCKL_4u64(x2, x3);	// y2 = l20 l30  l22 l32
  y3 = #VPUNPCKH_4u64(x2, x3);	// y3 = l21 l31  l23 l33

  x0 = #VPERM2I128(y0, y2, 0x20);	// x0 = l00 l10  l20 l30
  x1 = #VPERM2I128(y1, y3, 0x20);	// x1 = l01 l11  l21 l31
  x2 = #VPERM2I128(y0, y2, 0x31);	// x2 = l02 l12  l22 l32
  x3 = #VPERM2I128(y1, y3, 0x31);	// x3 = l03 l13  l23 l33

  return x0, x1, x2, x3;
}

// extracts 4 keccak states (st25) from a 4-way state (st4x)
inline fn __4u64x4_u256x4
( reg u256 y0 y1 y2 y3
) -> reg u256, reg u256, reg u256, reg u256 {
  // y0 = l00 l10  l20 l30
  // y1 = l01 l11  l21 l31
  // y2 = l02 l12  l22 l32
  // y3 = l03 l13  l23 l33
  reg u256 x0, x1, x2, x3;
  x0 = #VPERM2I128(y0, y2, 0x20);	// x0 = l00 l10  l02 l12
  x1 = #VPERM2I128(y1, y3, 0x20);	// x1 = l01 l11  l03 l13
  x2 = #VPERM2I128(y0, y2, 0x31);	// x2 = l20 l30  l22 l32
  x3 = #VPERM2I128(y1, y3, 0x31);	// x3 = l21 l31  l23 l33

  y0 = #VPUNPCKL_4u64(x0, x1);	// y0 = l00 l01  l02 l03
  y1 = #VPUNPCKH_4u64(x0, x1);	// y1 = l10 l11  l12 l13
  y2 = #VPUNPCKL_4u64(x2, x3);	// y2 = l20 l21  l22 l23
  y3 = #VPUNPCKH_4u64(x2, x3);	// y3 = l30 l31  l32 l33

  return y0, y1, y2, y3;
}

/*
   INCREMENTAL (FIXED-SIZE) MEMORY BROADCAST ABSORB
   ================================================
*/

inline fn __addstate_bcast_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT /* bytes (0 <= AT < 200) */
, reg u64 buf
, inline int LEN
, inline int TRAILB
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* buf */
{
  inline int LO, ALL;
  reg u64 at;
  reg u256 t256;

  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = 32 * (AT / 8); // current pstate position

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      buf, _, TRAILB, t256 = __mread_bcast_4subu64(buf, LEN, TRAILB);
      t256 ^= st.[u256 (int) at];
      st.[u256 (int) at] = t256;
      LO = 0;
      AT = 0;
      LEN = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
        t256 = #VPBROADCAST_4u64((u64)[buf]);
        buf += (8-LO);
      } else {
        buf, _, _, t256 = __mread_bcast_4subu64(buf, 8-LO, 0);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t256 = #VPSLL_4u64(t256, 8*LO);
      t256 ^= st.[u256 (int) at];
      st.[u256 (int) at] = t256;
      at += 8;
    }
  }

  // continue processing remaining bytes
  if (8 <= LEN) {
    while ( at < 32*(AT/8)+32*(LEN/8)) {
      t256 = #VPBROADCAST_4u64((u64)[buf]);
      buf += 8;
      t256 ^= st.[u256 at];
      st.[u256 at] = t256;
      at += 32;
    }
    LEN = LEN % 8;
  }

  // process last word (possibly closing the state)
  LO = ALL % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    if ( TRAILB != 0 ) { ALL += 1; }
    buf, _, TRAILB, t256 = __mread_bcast_4subu64(buf, LO, TRAILB);
    t256 ^= st[u256 (ALL/8)];
    st[u256 (ALL/8)] = t256;
  }
    
  return st, ALL, buf;
} 

inline fn __absorb_bcast_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT
, reg u64 buf
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* buf */
{
  reg u64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    st, AT, buf = __addstate_bcast_imem_avx2x4(st, AT, buf, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      st, _, buf = __addstate_bcast_imem_avx2x4(st, AT, buf, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      st = _keccakf1600_avx2x4(st);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, _, buf = __addstate_bcast_imem_avx2x4(st, 0, buf, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    st, AT, buf = __addstate_bcast_imem_avx2x4(st, 0, buf, LEN, TRAILB);
    if (TRAILB!=0) { st = __addratebit_avx2x4(st, RATE8); }
  }
  return st, AT, buf;
} 

/*
   INCREMENTAL (FIXED-SIZE) MEMORY 4-way ABSORB
   ============================================
*/
/*
inline fn __addstate_imem_avx2x4
( reg mut ptr u64[25] st
, inline int AT /* bytes (0 <= AT < 200) */
, reg u64 buf0 buf1 buf2 buf3
, inline int LEN
, inline int TRAILB
) -> reg ptr u64[25] /* st */
   , inline int /* AT */
   , reg u64 /* buf0 */
   , reg u64 /* buf1 */
   , reg u64 /* buf2 */
   , reg u64 /* buf3 */
{
  inline int LO, ALL;
  reg u64 at, t64;
  reg u256 t256;
  reg u128 t128;

  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = AT / 8; // current state position

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      buf, _, TRAILB, t256 = __mread_bcast_4subu64(buf, LEN, TRAILB);
      st[(int) at] ^= t256;
      LO = 0;
      AT = 0;
      LEN = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
        t256 = #VPBROADCAST_4u64((u64)[buf]);
        buf += (8-LO);
      } else {
        buf, _, _, t256 = __mread_bcast_4subu64(buf, 8-LO, 0);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t256 = #VPSLL_4u64(t256, 8*LO);
      st[(int) at] ^= t256;
      at += 1;
    }
  }

  // continue processing remaining bytes
  if (8 <= LEN) {
    while ( at < AT/8+(LEN/8)) {
      t256 = #VPBROADCAST_4u64((u64)[buf]);
      buf += 8;
      st[(int) at] ^= t256;
      at += 1;
    }
    LEN = LEN % 8;
  }

  // process last word (possibly closing the state)
  LO = ALL % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    if ( TRAILB != 0 ) { ALL += 1; }
    buf, _, TRAILB, t256 = __mread_bcast_4subu64(buf, LO, TRAILB);
    st[u64 (ALL/8)] ^= t256;
  }
    
  return st, ALL, buf;
} 

inline fn __absorb_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT
, reg u64 buf
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg u64 /* buf */
{
  reg u64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    st, AT, buf = __addstate_bcast_imem_avx2x4(st, AT, buf, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      st, _, buf = __addstate_bcast_imem_avx2x4(st, AT, buf, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      st = _keccakf1600_avx2x4(st);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, _, buf = __addstate_bcast_imem_avx2x4(st, 0, buf, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    st, AT, buf = __addstate_bcast_imem_avx2x4(st, 0, buf, LEN, TRAILB);
    if (TRAILB!=0) { st = __addratebit_avx2x4(st, RATE8); }
  }
  return st, AT, st, buf;
} 
*/

/*
   ONE-SHOT (FIXED-SIZE) MEMORY SQUEEZE
   ====================================
*/
inline fn __dumpstate_imem_avx2x4
( reg u64 buf0 buf1 buf2 buf3
, inline int LEN
, reg const ptr u256[25] st
) -> reg u64 /* buf0 */
   , reg u64 /* buf1 */
   , reg u64 /* buf2 */
   , reg u64 /* buf3 */
{
  reg u256 x0, x1, x2, x3;
  reg u64 i, t0, t1, t2, t3;
  i = 0;
  while (i <s 32*(LEN/32)) {
    x0 = st.[u256 4*i+0*32];
    x1 = st.[u256 4*i+1*32];
    x2 = st.[u256 4*i+2*32];
    x3 = st.[u256 4*i+3*32];
    i += 32;
    x0, x1, x2, x3 = __4u64x4_u256x4(x0, x1, x2, x3);
    (u256)[buf0] = x0;
    buf0 +=32;
    (u256)[buf1] = x1;
    buf1 +=32;
    (u256)[buf2] = x2;
    buf2 +=32;
    (u256)[buf3] = x3;
    buf3 +=32;
  } // 0 32 (64) 
  while (i <s 8*(LEN/8)) {
    t0 = st.[u64 4*i+0*8];
    t1 = st.[u64 4*i+1*8];
    t2 = st.[u64 4*i+2*8];
    t3 = st.[u64 4*i+3*8];
    i += 8;
    (u64)[buf0] = t0;
    buf0 += 8;
    (u64)[buf1] = t1;
    buf1 += 8;
    (u64)[buf2] = t2;
    buf2 += 8;
    (u64)[buf3] = t3;
    buf3 += 8;
  }

  if (0 < LEN%8) {
    t0 = st.[u64 4*i+0*8];
    t1 = st.[u64 4*i+1*8];
    t2 = st.[u64 4*i+2*8];
    t3 = st.[u64 4*i+3*8];
    buf0, _ = __mwrite_subu64( buf0, LEN%8, t0);
    buf1, _ = __mwrite_subu64( buf1, LEN%8, t1);
    buf2, _ = __mwrite_subu64( buf2, LEN%8, t2);
    buf3, _ = __mwrite_subu64( buf3, LEN%8, t3);
  }

  return buf0, buf1, buf2, buf3;
}

inline fn __squeeze_imem_avx2x4
( reg u64 buf0 buf1 buf2 buf3
, inline int LEN
, reg mut ptr u256[25] st
, inline int RATE8
) -> reg u64 /* buf0 */
   , reg u64 /* buf1 */
   , reg u64 /* buf2 */
   , reg u64 /* buf3 */
   , reg ptr u256[25] /* st */
{
  reg u64 i;
  inline int ITERS, LO;
  ITERS = LEN/RATE8;
  LO = LEN%RATE8;
  if (0 <s LEN) {
    if (0 < ITERS) {
      i = 0;
      while (i < ITERS) {
        st = _keccakf1600_avx2x4(st);
        buf0, buf1, buf2, buf3
          = __dumpstate_imem_avx2x4(buf0, buf1, buf2, buf3, RATE8, st);
        i += 1;
      }
    }
    if (0 < LO) {
        st = _keccakf1600_avx2x4(st);
        buf0, buf1, buf2, buf3
          = __dumpstate_imem_avx2x4(buf0, buf1, buf2, buf3, LO, st);
    }
  }
  return buf0, buf1, buf2, buf3, st;
}

