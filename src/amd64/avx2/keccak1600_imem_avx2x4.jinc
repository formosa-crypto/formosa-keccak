require "keccak1600_avx2x4.jinc"


/*
   INCREMENTAL (FIXED-SIZE) MEMORY BROADCAST ABSORB
   ================================================
*/

inline fn __addstate_bcast_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT /* bytes (0 <= AT < 200) */
, reg ui64 buf
, inline int LEN
, inline int TRAILB
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg ui64 /* buf */
requires{0 <= LEN &&  0<=AT  && AT < 200 && AT + LEN + (TRAILB != 0 ? 1 : 0) < 200 
      && is_mem_init((64u) buf,LEN) && is_arr_init(st,0,25*32)}
requires{0 <= TRAILB && TRAILB < 256}
ensures{is_arr_init(result.0,0,25*32) && result.1 == AT + LEN + (TRAILB != 0 ? 1 : 0) && result.2 == buf + LEN}
{
  inline int LO, ALL;
  reg ui64 at;
  reg u256 t256;

  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = 32 * (AT / 8); // current pstate position

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      buf, _, TRAILB, t256 = __mread_bcast_4subu64(buf, LEN, TRAILB);
      t256 = #VPSLL_4u64(t256, 8*LO);
      t256 ^= st.[u256 at];
      st.[u256 at] = t256;
      LO = 0;
      AT = 0;
      LEN = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
        t256 = #VPBROADCAST_4u64((u64)[(64u) buf]);
        buf += (8-LO);
      } else {
        buf, _, _, t256 = __mread_bcast_4subu64(buf, 8-LO, 0);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t256 = #VPSLL_4u64(t256, 8*LO);
      t256 ^= st.[u256 at];
      st.[u256 at] = t256;
      at += 32;
    }
  }

  // continue processing remaining bytes
  if (8 <= LEN) {
    while ( at < 32*(AT/8)+32*(LEN/8)) {
      t256 = #VPBROADCAST_4u64((u64)[(64u) buf]);
      buf += 8;
      t256 ^= st.[u256 at];
      st.[u256 at] = t256;
      at += 32;
    }
    LEN = (AT+LEN) % 8;
  }

  // process last word (possibly closing the state)
  LO = (AT+LEN) % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    if ( TRAILB != 0 ) { ALL += 1; }
    buf, _, TRAILB, t256 = __mread_bcast_4subu64(buf, LO, TRAILB);
    t256 ^= st.[u256 at];
    st.[u256 at] = t256;
  }
    
  return st, ALL, buf;
} 

inline fn __absorb_bcast_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT
, reg ui64 buf
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg ui64 /* buf */
requires{0 <= LEN &&  0<=AT  && AT < RATE8 && AT + LEN + (TRAILB != 0 ? 1 : 0) < 200 && 0 < RATE8 && RATE8 < 200 
      && is_mem_init((64u) buf,LEN) && is_arr_init(st,0,25*32)}
requires{0 <= TRAILB && TRAILB < 256}
ensures{is_arr_init(result.0,0,25*32) && result.1 == AT + LEN + (TRAILB != 0 ? 1 : 0)}
{
  reg ui64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    st, AT, buf = __addstate_bcast_imem_avx2x4(st, AT, buf, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      st, _, buf = __addstate_bcast_imem_avx2x4(st, AT, buf, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      //() = #spill(buf);
      st = _keccakf1600_avx2x4(st);
      //() = #unspill(buf);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, _, buf = __addstate_bcast_imem_avx2x4(st, 0, buf, RATE8, 0);
      //() = #spill(buf,i);
      st = _keccakf1600_avx2x4(st);
      //() = #unspill(buf,i);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    st, AT, buf = __addstate_bcast_imem_avx2x4(st, 0, buf, LEN, TRAILB);
    if (TRAILB!=0) { st = __addratebit_avx2x4(st, RATE8); }
  }
  return st, AT, buf;
} 

/*
   INCREMENTAL (FIXED-SIZE) MEMORY 4-way ABSORB
   ============================================
*/

inline fn __addstate_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT /* bytes (0 <= AT < 200) */
, reg ui64 buf0 buf1 buf2 buf3
, inline int LEN
, inline int TRAILB
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg ui64 /* buf0 */
   , reg ui64 /* buf1 */
   , reg ui64 /* buf2 */
   , reg ui64 /* buf3 */
requires{0 <= LEN && 0<=AT && AT < 200 && 
         AT + LEN + (TRAILB != 0 ? 1 : 0) < 200 &&
         is_mem_init((64u) buf0,LEN) && is_mem_init((64u) buf1,LEN) && 
         is_mem_init((64u) buf2,LEN) && is_mem_init((64u) buf3,LEN) && 
         is_arr_init(st,0,25*32)}
requires{0 <= TRAILB && TRAILB < 256}
ensures{is_arr_init(result.0,0,25*32) && 
          result.1 == AT + LEN + (TRAILB != 0 ? 1 : 0) &&
          result.2 == buf0 + LEN && result.3 == buf1 + LEN  &&
          result.4 == buf2 + LEN && result.5 == buf3 + LEN 
}
{
  inline int LO, ALL;
  reg ui64 at;
  reg u64 t0, t1, t2, t3;
  reg u256 t256_0, t256_1, t256_2, t256_3;

  ALL = AT+LEN; // total bytes to process (excluding trail byte, if !=0)
  LO = AT % 8; // leftover bytes
  at = 4 * (AT / 8); // current pstate position (referencing u64 words)
//at = 0, 4, 8, ...

  if ( 0 < LO ) { // process first word...
    if ( LO+LEN < 8) { // ...not enough to fill a word (just update it)
      if ( TRAILB != 0 ) { ALL += 1; }
      buf0, _, _, t0 = __mread_subu64(buf0, LEN, TRAILB);
      buf1, _, _, t1 = __mread_subu64(buf1, LEN, TRAILB);
      buf2, _, _, t2 = __mread_subu64(buf2, LEN, TRAILB);
      buf3, _, _, t3 = __mread_subu64(buf3, LEN, TRAILB);
      t0 <<= 8*LO;
      t0 ^= st[u64 at + 0];
      st[u64 at + 0] = t0;
      t1 <<= 8*LO;
      t1 ^= st[u64 at + 1];
      st[u64 at + 1] = t1;
      t2 <<= 8*LO;
      t2 ^= st[u64 at + 2];
      st[u64 at + 2] = t2;
      t3 <<= 8*LO;
      t3 ^= st[u64 at + 3];
      st[u64 at + 3] = t3;
      LO = 0;
      AT = 0;
      LEN = 0;
      TRAILB = 0;
    } else { // process first word
      if ( 8 <= LEN ) {
	t0 = (u64)[(64u) buf0];
	buf0 += 8-LO;
	t1 = (u64)[(64u) buf1];
	buf1 += 8-LO;
	t2 = (u64)[(64u) buf2];
	buf2 += 8-LO;
	t3 = (u64)[(64u) buf3];
	buf3 += 8-LO;
      } else {
        buf0, _, _, t0 = __mread_subu64(buf0, 8-LO, TRAILB);
        buf1, _, _, t1 = __mread_subu64(buf1, 8-LO, TRAILB);
        buf2, _, _, t2 = __mread_subu64(buf2, 8-LO, TRAILB);
        buf3, _, _, t3 = __mread_subu64(buf3, 8-LO, TRAILB);
      }
      LEN -= 8-LO;
      AT += 8-LO;
      t0 <<= 8*LO;
      t0 ^= st[u64 at + 0];
      st[u64 at + 0] = t0;
      t1 <<= 8*LO;
      t1 ^= st[u64 at + 1];
      st[u64 at + 1] = t1;
      t2 <<= 8*LO;
      t2 ^= st[u64 at + 2];
      st[u64 at + 2] = t2;
      t3 <<= 8*LO;
      t3 ^= st[u64 at + 3];
      st[u64 at + 3] = t3;
      at += 4;
    }
  }

  // continue processing remaining bytes
  if (8 <= LEN) {
    while ( at < 4*(AT/8)+32*(LEN/32) ) {
      t256_0 = (u256)[(64u) buf0];
      buf0 += 32;
      t256_1 = (u256)[(64u) buf1];
      buf1 += 32;
      t256_2 = (u256)[(64u) buf2];
      buf2 += 32;
      t256_3 = (u256)[(64u) buf3];
      buf3 += 32;
      t256_0, t256_1, t256_2, t256_3 = __4u64x4_u256x4(t256_0, t256_1, t256_2, t256_3);
      st.[u256 8*at] = t256_0;
      st.[u256 8*at+32] = t256_1;
      st.[u256 8*at+64] = t256_2;
      st.[u256 8*at+96] = t256_3;
      at += 32;
    }
    while ( at < 4*(AT/8)+4*(LEN/8)) {
      t0 = (u64)[(64u) buf0];
      buf0 += 8;
      t0 ^= st[u64 at + 0];
      st[u64 at + 0] = t0;
      t1 = (u64)[(64u) buf1];
      buf1 += 8;
      t1 ^= st[u64 at + 1];
      st[u64 at + 1] = t1;
      t2 = (u64)[(64u) buf2];
      buf2 += 8;
      t2 ^= st[u64 at + 2];
      st[u64 at + 2] = t2;
      t3 = (u64)[(64u) buf3];
      buf3 += 8;
      t3 ^= st[u64 at + 3];
      st[u64 at + 3] = t3;
      at += 4;
    }
    LEN = (AT+LEN) % 8;
  }

  // process last word (possibly closing the state)
  LO = (AT+LEN) % 8;
  if ( 0 < LO || TRAILB != 0 ) {
    buf0, _, _, t0 = __mread_subu64(buf0, LO, TRAILB);
    buf1, _, _, t1 = __mread_subu64(buf1, LO, TRAILB);
    buf2, _, _, t2 = __mread_subu64(buf2, LO, TRAILB);
    buf3, _, _, t3 = __mread_subu64(buf3, LO, TRAILB);
    if ( TRAILB != 0 ) { ALL += 1; TRAILB = 0; }
    t0 ^= st[u64 at + 0];
    st[u64 at + 0] = t0;
    t1 ^= st[u64 at + 1];
    st[u64 at + 1] = t1;
    t2 ^= st[u64 at + 2];
    st[u64 at + 2] = t2;
    t3 ^= st[u64 at + 3];
    st[u64 at + 3] = t3;
  }
    
  return st, ALL, buf0, buf1, buf2, buf3;
} 


inline fn __absorb_imem_avx2x4
( reg mut ptr u256[25] st
, inline int AT
, reg ui64 buf0
, reg ui64 buf1
, reg ui64 buf2
, reg ui64 buf3
, inline int LEN
, inline int RATE8
, inline int TRAILB /* closes state if !=0 (i.e. adds trailbyte and padding) */
) -> reg ptr u256[25] /* st */
   , inline int /* AT */
   , reg ui64 /* buf0 */
   , reg ui64 /* buf1 */
   , reg ui64 /* buf2 */
   , reg ui64 /* buf3 */
requires{0 <= LEN && 0<=AT && AT < RATE8 && 0 < RATE8 && RATE8 < 200 &&
         AT + LEN + (TRAILB != 0 ? 1 : 0) < 200 &&
         is_mem_init((64u) buf0,LEN) && is_mem_init((64u) buf1,LEN) && 
         is_mem_init((64u) buf2,LEN) && is_mem_init((64u) buf3,LEN) && 
         is_arr_init(st,0,25*32)}
requires{0 <= TRAILB && TRAILB < 256}
ensures{is_arr_init(result.0,0,25*32)}
{
  reg ui64 i;
  inline int ALL, ITERS;

  ALL = AT + LEN;
  if ( (AT+LEN) < RATE8 ) { // not enough to fill a block!
    st, AT, buf0, buf1, buf2, buf3
      = __addstate_imem_avx2x4(st, AT, buf0, buf1, buf2, buf3, LEN, TRAILB);
    if (TRAILB != 0) { // add pstate and closes the state
      st = __addratebit_avx2x4(st, RATE8);
    }
  } else { // at least a block is filled
    if ( AT != 0 ) { // start by filling the first block
      st, _, buf0, buf1, buf2, buf3
        = __addstate_imem_avx2x4(st, AT, buf0, buf1, buf2, buf3, RATE8-AT, 0);
      LEN = LEN - (RATE8-AT);
      st = _keccakf1600_avx2x4(st);
      AT = 0;
    }

    // continue by processing full blocks
    ITERS = LEN / RATE8; // number of full blocks
    i = 0;
    while ( i < ITERS ) {
      st, _, buf0, buf1, buf2, buf3
        = __addstate_imem_avx2x4(st, 0, buf0, buf1, buf2, buf3, RATE8, 0);
      st = _keccakf1600_avx2x4(st);
      i += 1;
    }

    // last incomplete block
    LEN = ALL % RATE8;
    st, AT, buf0, buf1, buf2, buf3
      = __addstate_imem_avx2x4(st, 0, buf0, buf1, buf2, buf3, LEN, TRAILB);
    if (TRAILB!=0) { st = __addratebit_avx2x4(st, RATE8); }
  }
  return st, AT, buf0, buf1, buf2, buf3;
} 


/*
   ONE-SHOT (FIXED-SIZE) MEMORY SQUEEZE
   ====================================
*/
inline fn __dumpstate_imem_avx2x4
( reg ui64 buf0 buf1 buf2 buf3
, inline int LEN
, reg const ptr u256[25] st
) -> reg ui64 /* buf0 */
   , reg ui64 /* buf1 */
   , reg ui64 /* buf2 */
   , reg ui64 /* buf3 */
requires{0 <= LEN && 
         is_mem_init((64u) buf0,LEN) && is_mem_init((64u) buf1,LEN) && 
         is_mem_init((64u) buf2,LEN) && is_mem_init((64u) buf3,LEN) && 
         is_arr_init(st,0,25*32)}
ensures{result.0 == buf0 + LEN && result.1 == buf1 + LEN &&
        result.2 == buf2 + LEN && result.3 == buf3 + LEN}
{
  reg u256 x0, x1, x2, x3;
  reg u64 i, t0, t1, t2, t3;
  i = 0;
  while (i <s 32*(LEN/32)) {
    x0 = st.[u256 4*i+0*32];
    x1 = st.[u256 4*i+1*32];
    x2 = st.[u256 4*i+2*32];
    x3 = st.[u256 4*i+3*32];
    i += 32;
    x0, x1, x2, x3 = __4u64x4_u256x4(x0, x1, x2, x3);
    (u256)[(64u) buf0] = x0;
    buf0 +=32;
    (u256)[(64u) buf1] = x1;
    buf1 +=32;
    (u256)[(64u) buf2] = x2;
    buf2 +=32;
    (u256)[(64u) buf3] = x3;
    buf3 +=32;
  } // 0 32 (64) 
  while (i <s 8*(LEN/8)) {
    t0 = st.[u64 4*i+0*8];
    t1 = st.[u64 4*i+1*8];
    t2 = st.[u64 4*i+2*8];
    t3 = st.[u64 4*i+3*8];
    i += 8;
    (u64)[(64u) buf0] = t0;
    buf0 += 8;
    (u64)[(64u) buf1] = t1;
    buf1 += 8;
    (u64)[(64u) buf2] = t2;
    buf2 += 8;
    (u64)[(64u) buf3] = t3;
    buf3 += 8;
  }

  if (0 < LEN%8) {
    t0 = st.[u64 4*i+0*8];
    t1 = st.[u64 4*i+1*8];
    t2 = st.[u64 4*i+2*8];
    t3 = st.[u64 4*i+3*8];
    buf0, _ = __mwrite_subu64( buf0, LEN%8, t0);
    buf1, _ = __mwrite_subu64( buf1, LEN%8, t1);
    buf2, _ = __mwrite_subu64( buf2, LEN%8, t2);
    buf3, _ = __mwrite_subu64( buf3, LEN%8, t3);
  }

  return buf0, buf1, buf2, buf3;
}

inline fn __squeeze_imem_avx2x4
( reg ui64 buf0 buf1 buf2 buf3
, inline int LEN
, reg mut ptr u256[25] st
, inline int RATE8
) -> reg ui64 /* buf0 */
   , reg ui64 /* buf1 */
   , reg ui64 /* buf2 */
   , reg ui64 /* buf3 */
   , reg ptr u256[25] /* st */
requires{0 <= LEN && 0 < RATE8 && RATE8 < 200 &&
      is_mem_init((64u) buf0,LEN) && is_mem_init((64u) buf1,LEN) && 
      is_mem_init((64u) buf2,LEN) && is_mem_init((64u) buf3,LEN) && 
      is_arr_init(st,0,25*32)}
ensures{is_arr_init(result.4,0,25*32) && 
        result.0 == buf0 + LEN && result.1 == buf1 + LEN &&
        result.2 == buf2 + LEN && result.3 == buf3 + LEN}
{
  reg ui64 i;
  inline int ITERS, LO;
  ITERS = LEN/RATE8;
  LO = LEN%RATE8;
  if (0 <s LEN) {
    if (0 < ITERS) {
      i = 0;
      while (i < ITERS) {
	//() = #spill(buf,i);
        st = _keccakf1600_avx2x4(st);
	//() = #unspill(buf,i);
        buf0, buf1, buf2, buf3
          = __dumpstate_imem_avx2x4(buf0, buf1, buf2, buf3, RATE8, st);
        i += 1;
      }
    }
    if (0 < LO) {
	//() = #spill(buf);
        st = _keccakf1600_avx2x4(st);
	//() = #unspill(buf);
        buf0, buf1, buf2, buf3
          = __dumpstate_imem_avx2x4(buf0, buf1, buf2, buf3, LO, st);
    }
  }
  return buf0, buf1, buf2, buf3, st;
}

